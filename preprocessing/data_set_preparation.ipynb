{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from preprocessing.data_preparation import read_data\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "# merge items and train data to a dataFrame\n",
    "def merge_data(input_path, items_path, output_path):\n",
    "    tdf = read_data(input_path)\n",
    "    idf = read_data(items_path)\n",
    "    print('data read successfully!')\n",
    "    output = Path(output_path)\n",
    "    if not output.is_file():\n",
    "        train_merged = tdf.copy()\n",
    "        train_merged = train_merged.merge(tdf.merge(idf, how='left', on='pid', sort=False))\n",
    "        pd.to_pickle(train_merged, output_path)\n",
    "        return train_merged\n",
    "    else:\n",
    "        return pd.read_pickle(output_path)\n",
    "\n",
    "\n",
    "def extract_numbers_from_content(input):\n",
    "    x_index = input.find('X')\n",
    "    if input == 'L   125':\n",
    "        return 1, 1, 125\n",
    "    if x_index == -1:\n",
    "        if input == 'PAK':\n",
    "            return 1, 1, 1\n",
    "        return 1, 1, input\n",
    "    second_part = input[x_index + 1: len(input)]\n",
    "    x_second_index = second_part.find('X')\n",
    "    if x_second_index == -1:\n",
    "        return 1, input[0: x_index], second_part\n",
    "    return input[0: x_index], second_part[0: x_second_index], second_part[x_second_index + 1: len(second_part)]\n",
    "\n",
    "\n",
    "unit_map = {\n",
    "    'KG': 1000,\n",
    "    'ST': 6350,\n",
    "    'P': 454,\n",
    "    'M': 100,\n",
    "    'L': 1000,\n",
    "    'G': 1,\n",
    "    'CM': 1,\n",
    "    'ML': 1,\n",
    "}\n",
    "\n",
    "\n",
    "def unit_converter(row):\n",
    "    return row['content_3'] * unit_map[row['unit']]\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    output = Path('data/unit_fixed.pkl')\n",
    "    if not output.is_file():\n",
    "        # example of using merge_data function for train dataset\n",
    "        mrg = merge_data('data/train.csv', 'data/items.csv', 'data/train_merged.pkl')\n",
    "\n",
    "        # add count feature (revenue/price)\n",
    "        mrg['count'] = mrg.revenue / mrg.price\n",
    "\n",
    "        # uppercase all pharmForm values\n",
    "        mrg['pharmForm'] = mrg['pharmForm'].str.upper()\n",
    "        # extract pharmForm values as binary feature and adding them to dataset\n",
    "        mrg = pd.concat([mrg, pd.get_dummies(mrg['pharmForm'])], axis=1)\n",
    "        mrg = mrg.drop('pharmForm', 1)\n",
    "\n",
    "        # split count of packs and amount of each to separate columns\n",
    "        extracted_numbers = mrg['content'].apply(extract_numbers_from_content)\n",
    "        extracted_numbers = pd.DataFrame(extracted_numbers.tolist(), columns=['content_1', 'content_2', 'content_3'],\n",
    "                                         index=extracted_numbers.index)\n",
    "        extracted_numbers['content_1'] = pd.to_numeric(extracted_numbers['content_1'])\n",
    "        extracted_numbers['content_2'] = pd.to_numeric(extracted_numbers['content_2'])\n",
    "        extracted_numbers['content_3'] = pd.to_numeric(extracted_numbers['content_3'])\n",
    "        mrg = pd.concat([mrg, extracted_numbers], axis=1)\n",
    "        mrg = mrg.drop('content', 1)\n",
    "\n",
    "        mrg['content_3'] = mrg.apply(unit_converter, axis=1)\n",
    "        mapping = {'KG': 'G', 'ST': 'G', 'P': 'G', 'L': 'ML', 'M': 'CM'}\n",
    "        mrg = mrg.replace({'unit': mapping})\n",
    "        pd.to_pickle(mrg, '../data/unit_fixed.pkl')\n",
    "        print('units converted')\n",
    "    else:\n",
    "        mrg = pd.read_pickle('data/unit_fixed.pkl')\n",
    "\n",
    "    # fill campaignIndex with D and then get dummy binary values of each category index\n",
    "    # mrg['campaignIndex'].fillna('D', inplace=True)\n",
    "    # mrg = pd.concat([mrg, pd.get_dummies(mrg['campaignIndex'])], axis=1)\n",
    "\n",
    "    # mrg = pd.concat([mrg, pd.get_dummies(mrg['group'])], axis=1)\n",
    "    return mrg\n",
    "\n",
    "\n",
    "def predict_competitor(all_data):\n",
    "    train = all_data[pd.notnull(all_data['competitorPrice'])]\n",
    "    kf = KFold(n_splits=10)\n",
    "    estimator = XGBRegressor()\n",
    "    x = train.drop('competitorPrice', 1)\n",
    "    y = train['competitorPri1ce']\n",
    "    scores = cross_val_score(estimator,\n",
    "                             x,\n",
    "                             y,\n",
    "                             cv=kf,\n",
    "                             scoring=make_scorer(mean_squared_error))\n",
    "    print(scores)\n",
    "\n",
    "\n",
    "data = prepare_dataset()\n",
    "# from scipy.stats import pearsonr\n",
    "#\n",
    "# print(data['category'].fillna(0))\n",
    "# print(pearsonr(data['category'].fillna(0), data['count']))\n",
    "\n",
    "\n",
    "# TODO handle features: category, group, competitor\n",
    "# TODO Random Forrest on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_missing = data[pd.isnull(data['campaignIndex'])]['lineID']\n",
    "adFlag_missing = data[data['adFlag'] == 0]['lineID']\n",
    "# print(campaign_missing)\n",
    "# print(data['adFlag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2287968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880176\n"
     ]
    }
   ],
   "source": [
    "print(len(campaign_missing))\n",
    "print(len(adFlag_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n"
     ]
    }
   ],
   "source": [
    "intersections = pd.Series(\n",
    "    list(set(campaign_missing).intersection(\n",
    "        set(adFlag_missing))))\n",
    "print(len(intersections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsp/anaconda3/lib/python3.5/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n  warn('The line search algorithm did not converge', LineSearchWarning)\n/home/dsp/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be filled with similar product campaign Index\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='newton-cg',\n",
    "                        multi_class='multinomial')\n",
    "train_data = data[pd.notnull(data['campaignIndex'])]\n",
    "lr.fit(train_data[['pid', 'manufacturer']],\n",
    "       train_data['campaignIndex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the 600K part\n",
    "pred = lr.predict(data[pd.isnull(\n",
    "    data['campaignIndex'])][['pid', 'manufacturer']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B']\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(data[pd.notnull(data['campaignIndex'])][['pid', 'manufacturer']])\n",
    "print(pd.Series(pred).unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_bayes_clf = GaussianNB()\n",
    "test_data = data[pd.isnull(data['campaignIndex'])]\n",
    "naive_bayes_clf.fit(train_data[['pid', 'manufacturer', 'rrp']],\n",
    "                    train_data['campaignIndex'])\n",
    "predictions = naive_bayes_clf.predict(\n",
    "    test_data[['pid', 'manufacturer', 'rrp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid fill value with a <class 'numpy.ndarray'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b654bc66b5da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'campaignIndex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dsp/anaconda3/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   2368\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m                                           \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m                                           **kwargs)\n\u001b[0m\u001b[1;32m   2371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shift'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsp/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m                     raise ValueError(\"invalid fill value with a %s\" %\n\u001b[0;32m-> 3266\u001b[0;31m                                      type(value))\n\u001b[0m\u001b[1;32m   3267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m                 new_data = self._data.fillna(value=value, limit=limit,\n",
      "\u001b[0;31mValueError\u001b[0m: invalid fill value with a <class 'numpy.ndarray'>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "test_data['campaignIndex'].fillna(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n0          NaN\n1            C\n2          NaN\n3          NaN\n4          NaN\n5          NaN\n6          NaN\n7          NaN\n8          NaN\n9          NaN\n10           C\n11         NaN\n12         NaN\n13         NaN\n14         NaN\n15           A\n16           B\n17         NaN\n18         NaN\n19         NaN\n20         NaN\n21         NaN\n22         NaN\n23         NaN\n24           B\n25         NaN\n26         NaN\n27         NaN\n28         NaN\n29         NaN\n          ... \n2755973    NaN\n2755974    NaN\n2755975    NaN\n2755976    NaN\n2755977    NaN\n2755978    NaN\n2755979    NaN\n2755980    NaN\n2755981    NaN\n2755982    NaN\n2755983    NaN\n2755984    NaN\n2755985    NaN\n2755986    NaN\n2755987      B\n2755988    NaN\n2755989    NaN\n2755990    NaN\n2755991      B\n2755992    NaN\n2755993    NaN\n2755994    NaN\n2755995    NaN\n2755996    NaN\n2755997    NaN\n2755998    NaN\n2755999    NaN\n2756000    NaN\n2756001      A\n2756002      A\nName: campaignIndex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(intersections)\n",
    "# These are lineIDs with missing campaignIndex and adFlag=0\n",
    "print(len(intersections))\n",
    "ind = data.lineID.isin(intersections.tolist())\n",
    "print(len(data[ind]['campaignIndex']))\n",
    "print(data['campaignIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            D\n1            C\n2            D\n3          NaN\n4            D\n5            D\n6          NaN\n7            D\n8            D\n9          NaN\n10           C\n11         NaN\n12           D\n13           D\n14           D\n15           A\n16           B\n17           D\n18           D\n19           D\n20           D\n21           D\n22           D\n23           D\n24           B\n25           D\n26           D\n27           D\n28         NaN\n29           D\n          ... \n2755973      D\n2755974      D\n2755975      D\n2755976      D\n2755977      D\n2755978    NaN\n2755979      D\n2755980      D\n2755981      D\n2755982      D\n2755983      D\n2755984      D\n2755985    NaN\n2755986      D\n2755987      B\n2755988      D\n2755989      D\n2755990      D\n2755991      B\n2755992      D\n2755993      D\n2755994      D\n2755995      D\n2755996      D\n2755997    NaN\n2755998      D\n2755999      D\n2756000      D\n2756001      A\n2756002      A\nName: campaignIndex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# To be filled with D\n",
    "data['campaignIndex'].\\\n",
    "    fillna(data[ind]['campaignIndex'].fillna('D'), inplace=True)\n",
    "print(data['campaignIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c112e1d4a0cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dsp/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pd.DataFrame(pred).unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}